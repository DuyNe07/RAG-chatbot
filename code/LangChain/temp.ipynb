{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "print(faiss.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # True nếu GPU khả dụng\n",
    "print(torch.cuda.device_count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luong\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]c:\\Users\\luong\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\luong\\.cache\\huggingface\\hub\\models--TheBloke--Llama-2-7B-GGML. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [02:06<00:00, 126.59s/it]\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GGML\", gpu_layers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' change the world. AI is going to change the way we work, and even change what it means to be human.” – Riaan Carey\\n“AI is going to change the world. AI is going to change the way we work, and even change what it means to be human.”–Riaan Carey (see Fig 512 below).\\nFig 512: “AI is going to change the world. AI is going to change the way we work, and even change what it means to be human.” – Riaan Carey\\nAI will become so good at understanding people that you can just say a few words about yourself to your personal device. That personal assistant becomes smarter, based on learning patterns of how you behave in different situations. It’s like having a friend who lives and dies for every expression that might ever fall out of my lips as I interact with it.” – Riaan Carey\\n“AI will become so good at understanding people that you can just say a few words about yourself to your personal device. That personal assistant becomes smarter, based on learning patterns of how you behave in different situations. It’s like having a friend who lives and dies for every expression that'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"AI is going to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import torch\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA GeForce GTX 1650\n",
      "FAISS index is on: GPU\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import torch\n",
    "\n",
    "# Kiểm tra GPU có sẵn không\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU không khả dụng\")\n",
    "\n",
    "# Tạo một FAISS index trên CPU\n",
    "dim = 128  # Kích thước vector\n",
    "index_cpu = faiss.IndexFlatL2(dim)\n",
    "\n",
    "# Chuyển index lên GPU\n",
    "gpu_res = faiss.StandardGpuResources()\n",
    "index_gpu = faiss.index_cpu_to_gpu(gpu_res, 0, index_cpu)  # Chuyển lên GPU 0\n",
    "\n",
    "# Kiểm tra\n",
    "print(f\"FAISS index is on: {'GPU' if isinstance(index_gpu, faiss.GpuIndex) else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luong\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]c:\\Users\\luong\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\luong\\.cache\\huggingface\\hub\\models--TheBloke--Llama-2-7B-GGML. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [03:23<00:00, 203.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GGML\", gpu_layers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "llm(\"Viet Nam is a beautiful country \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
